{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091e428f",
   "metadata": {},
   "source": [
    "# 선형회귀\n",
    "## 훈련 데이터와 테스트 데이터셋\n",
    "\n",
    "![img](./img/img33.PNG)\n",
    "\n",
    "## 문제\n",
    "어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부했더니 6점을 맞았음\n",
    "그렇다면, 내가 4시간을 공부한다면 몇 점을 맞을 수 있을까?\n",
    "![img](./img/img34.PNG)\n",
    "\n",
    "## 가설 수립(Hypothesis)\n",
    "* 머신 러닝에서 식을 세울때 이 식을 가설(Hypothesis)라고 함\n",
    "* 보통 머신 러닝에서 가설은 임의로 추측해서 세워보는 식일수도 있고, 경험적으로 알고 있는 식일 수도 있움\n",
    "* 맞는 가설이 아니라고 판단되면 계속 수정해나가게 되는 식이기도 합니다.\n",
    "\n",
    "* 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일로 선형 회귀의 가설(직선의 방정식)은 아래와 같움\n",
    "\n",
    "$$y=Wx+b$$\n",
    "가설의 H를 따서\n",
    "$$H(x)=Wx+b$$\n",
    "이때, x와 곱해지는 W를 가중치라고하며 b를 편향이라고 함\n",
    "\n",
    "## 파이토치로 선형회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4110e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2789fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fccb809de90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 시드를 고정\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e988c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddda7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f5cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ddb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "# 가중치 W를 출력\n",
    "print(W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "453772ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "486b9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59dff749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6279214",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40faf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "# W와 b를 업데이트\n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f99bfd",
   "metadata": {},
   "source": [
    "## zero_grad()가 필요한 이유\n",
    "* 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0a17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "  z = 2*w\n",
    "\n",
    "  z.backward()\n",
    "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71622e2d",
   "metadata": {},
   "source": [
    "# 자동 미분\n",
    "* 경사 하강법은 비용 함수를 미분하여 이 함수의 기울기(gradient)를 구해서 비용이 최소화 되는 방향을 찾아내는 알고리즘\n",
    "\n",
    "* 비용 함수를 손실 함수, 오차 함수라고도 부르므로 비용이 최소화 되는 방향이라는 표현 대신 손실이 최소화 되는 방향 또는 오차를 최소화 되는 방향이라고 말할 수 있음\n",
    "* 모델이 복잡해질수록 경사 하강법을 넘파이 등으로 직접 코딩하는 것은 까다로움\n",
    "* 파이토치에서는 이런 수고를 하지 않도록 자동 미분(Autograd)을 지원\n",
    "* 자동 미분을 사용하면 미분 계산을 자동화하여 경사 하강법을 손쉽게 사용할 수 있게 해줌\n",
    "![img](./img/img35.png)\n",
    "\n",
    "$2w^2+5$라는 식을 세워보고, $w$에 대해 미분   $z=4w+5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9ab57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "z.backward()\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc72a83",
   "metadata": {},
   "source": [
    "# 다중 선형 회귀\n",
    "![img](./img/img37.PNG)\n",
    "\n",
    "$$H(x) = w_1x_1+w_2x_2+w_3x_3+b$$\n",
    "![img](./img/img38.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa33a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7260fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db16cba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed250f",
   "metadata": {},
   "source": [
    "$\\left(\n",
    "    \\begin{array}{c}\n",
    "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
    "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
    "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
    "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
    "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "      w_{1} \\\\\n",
    "      w_{2} \\\\\n",
    "      w_{3} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "+\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "      b \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    " \\ =\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3} + b\\ \\\\\n",
    "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3} + b\\ \\\\\n",
    "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3} + b\\ \\\\\n",
    "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3} + b\\ \\\\\n",
    "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3} + b\\ \\\\\n",
    "    \\end{array}\n",
    "  \\right)$\n",
    "  \n",
    "  $$H(X) = XW + B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "649ae12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
      "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n",
      "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
      "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
      "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
      "Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
      "Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
      "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
      "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
      "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
      "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
      "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
      "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
      "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
      "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
      "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
      "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
      "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
      "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
      "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"
     ]
    }
   ],
   "source": [
    "#행렬연산을 고려하여 구현\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33aa252",
   "metadata": {},
   "source": [
    "## nn.Module로 다중 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d266aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fccb809de90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3d43c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f15e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a760247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04514eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6f50c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31667.597656\n",
      "Epoch  100/2000 Cost: 0.225993\n",
      "Epoch  200/2000 Cost: 0.223911\n",
      "Epoch  300/2000 Cost: 0.221941\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218271\n",
      "Epoch  600/2000 Cost: 0.216575\n",
      "Epoch  700/2000 Cost: 0.214950\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211952\n",
      "Epoch 1000/2000 Cost: 0.210560\n",
      "Epoch 1100/2000 Cost: 0.209232\n",
      "Epoch 1200/2000 Cost: 0.207967\n",
      "Epoch 1300/2000 Cost: 0.206761\n",
      "Epoch 1400/2000 Cost: 0.205619\n",
      "Epoch 1500/2000 Cost: 0.204522\n",
      "Epoch 1600/2000 Cost: 0.203484\n",
      "Epoch 1700/2000 Cost: 0.202485\n",
      "Epoch 1800/2000 Cost: 0.201542\n",
      "Epoch 1900/2000 Cost: 0.200635\n",
      "Epoch 2000/2000 Cost: 0.199769\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dc83998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e13b0",
   "metadata": {},
   "source": [
    "## 클래스로 다중 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e08a501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf511a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fc171a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4925b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69fddb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 39633.414062\n",
      "Epoch  100/2000 Cost: 11.480746\n",
      "Epoch  200/2000 Cost: 10.894592\n",
      "Epoch  300/2000 Cost: 10.339335\n",
      "Epoch  400/2000 Cost: 9.813351\n",
      "Epoch  500/2000 Cost: 9.315010\n",
      "Epoch  600/2000 Cost: 8.842962\n",
      "Epoch  700/2000 Cost: 8.395753\n",
      "Epoch  800/2000 Cost: 7.972028\n",
      "Epoch  900/2000 Cost: 7.570637\n",
      "Epoch 1000/2000 Cost: 7.190376\n",
      "Epoch 1100/2000 Cost: 6.830142\n",
      "Epoch 1200/2000 Cost: 6.488811\n",
      "Epoch 1300/2000 Cost: 6.165472\n",
      "Epoch 1400/2000 Cost: 5.859105\n",
      "Epoch 1500/2000 Cost: 5.568909\n",
      "Epoch 1600/2000 Cost: 5.293931\n",
      "Epoch 1700/2000 Cost: 5.033408\n",
      "Epoch 1800/2000 Cost: 4.786575\n",
      "Epoch 1900/2000 Cost: 4.552718\n",
      "Epoch 2000/2000 Cost: 4.331151\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a669e",
   "metadata": {},
   "source": [
    "## 미니배치와 데이터 로드\n",
    "\n",
    "* 만약, 데이터가 수십만개 이상이라면 전체 데이터에 대해서 경사 하강법을 수행하는 것은 매우 느릴 뿐만 아니라 많은 계산량이 필요함\n",
    "* 정말 어쩌면 메모리의 한계로 계산이 불가능한 경우도 있을 수 있음\n",
    "\n",
    "* 그렇기 때문에 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념이 나오게 되었음\n",
    "* 이 단위를 미니 배치(Mini Batch)라고 함\n",
    "\n",
    "![img](./img/img39.PNG)\n",
    "\n",
    "## 이터레이션, 배치크기, 에포크\n",
    "![img](./img/img40.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e271ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "459586d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a3fad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be682450",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ff7575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d809a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ceb40ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 21981.667969\n",
      "Epoch    0/20 Batch 2/3 Cost: 9587.075195\n",
      "Epoch    0/20 Batch 3/3 Cost: 3180.415527\n",
      "Epoch    1/20 Batch 1/3 Cost: 656.754761\n",
      "Epoch    1/20 Batch 2/3 Cost: 208.265381\n",
      "Epoch    1/20 Batch 3/3 Cost: 60.448395\n",
      "Epoch    2/20 Batch 1/3 Cost: 20.464521\n",
      "Epoch    2/20 Batch 2/3 Cost: 4.721592\n",
      "Epoch    2/20 Batch 3/3 Cost: 7.182361\n",
      "Epoch    3/20 Batch 1/3 Cost: 1.647293\n",
      "Epoch    3/20 Batch 2/3 Cost: 0.378707\n",
      "Epoch    3/20 Batch 3/3 Cost: 1.522778\n",
      "Epoch    4/20 Batch 1/3 Cost: 1.254019\n",
      "Epoch    4/20 Batch 2/3 Cost: 2.134122\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.052373\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.230407\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.461842\n",
      "Epoch    5/20 Batch 3/3 Cost: 2.749334\n",
      "Epoch    6/20 Batch 1/3 Cost: 1.041582\n",
      "Epoch    6/20 Batch 2/3 Cost: 1.587178\n",
      "Epoch    6/20 Batch 3/3 Cost: 1.168184\n",
      "Epoch    7/20 Batch 1/3 Cost: 0.446067\n",
      "Epoch    7/20 Batch 2/3 Cost: 2.133462\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.879393\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.129102\n",
      "Epoch    8/20 Batch 2/3 Cost: 1.629957\n",
      "Epoch    8/20 Batch 3/3 Cost: 1.018610\n",
      "Epoch    9/20 Batch 1/3 Cost: 1.180956\n",
      "Epoch    9/20 Batch 2/3 Cost: 1.115691\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.002779\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.581377\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.573192\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.001429\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.606302\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.692013\n",
      "Epoch   11/20 Batch 3/3 Cost: 2.220071\n",
      "Epoch   12/20 Batch 1/3 Cost: 1.800132\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.189872\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.045114\n",
      "Epoch   13/20 Batch 1/3 Cost: 1.179914\n",
      "Epoch   13/20 Batch 2/3 Cost: 0.999486\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.452195\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.930212\n",
      "Epoch   14/20 Batch 2/3 Cost: 1.366732\n",
      "Epoch   14/20 Batch 3/3 Cost: 0.001217\n",
      "Epoch   15/20 Batch 1/3 Cost: 1.391140\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.711748\n",
      "Epoch   15/20 Batch 3/3 Cost: 1.394616\n",
      "Epoch   16/20 Batch 1/3 Cost: 2.574636\n",
      "Epoch   16/20 Batch 2/3 Cost: 1.108926\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.102229\n",
      "Epoch   17/20 Batch 1/3 Cost: 0.114795\n",
      "Epoch   17/20 Batch 2/3 Cost: 1.629926\n",
      "Epoch   17/20 Batch 3/3 Cost: 1.041903\n",
      "Epoch   18/20 Batch 1/3 Cost: 1.028184\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.601377\n",
      "Epoch   18/20 Batch 3/3 Cost: 2.028243\n",
      "Epoch   19/20 Batch 1/3 Cost: 0.602391\n",
      "Epoch   19/20 Batch 2/3 Cost: 2.355999\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.969715\n",
      "Epoch   20/20 Batch 1/3 Cost: 0.786738\n",
      "Epoch   20/20 Batch 2/3 Cost: 2.512165\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.014205\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64549717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.9222]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de219501",
   "metadata": {},
   "source": [
    "## 커스텀 데이터셋으로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b09ad3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-57-2b1fde135706>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-2b1fde135706>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    def __len__(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "    #데이터셋의 전처리를 해주는 부분\n",
    "    \n",
    "    def __len__(self):\n",
    "    #데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "    #데이터셋에서 특정 1개의 샘플을 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3403abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ecd784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e94d0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7943554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f87aca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 29319.011719\n",
      "Epoch    0/20 Batch 2/3 Cost: 7682.496094\n",
      "Epoch    0/20 Batch 3/3 Cost: 3114.664795\n",
      "Epoch    1/20 Batch 1/3 Cost: 809.167480\n",
      "Epoch    1/20 Batch 2/3 Cost: 300.709198\n",
      "Epoch    1/20 Batch 3/3 Cost: 37.518803\n",
      "Epoch    2/20 Batch 1/3 Cost: 37.871727\n",
      "Epoch    2/20 Batch 2/3 Cost: 3.583126\n",
      "Epoch    2/20 Batch 3/3 Cost: 15.499701\n",
      "Epoch    3/20 Batch 1/3 Cost: 3.772459\n",
      "Epoch    3/20 Batch 2/3 Cost: 2.178463\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.037612\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.297060\n",
      "Epoch    4/20 Batch 2/3 Cost: 2.369325\n",
      "Epoch    4/20 Batch 3/3 Cost: 2.928143\n",
      "Epoch    5/20 Batch 1/3 Cost: 1.220536\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.797108\n",
      "Epoch    5/20 Batch 3/3 Cost: 1.805787\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.409901\n",
      "Epoch    6/20 Batch 2/3 Cost: 2.198052\n",
      "Epoch    6/20 Batch 3/3 Cost: 2.823555\n",
      "Epoch    7/20 Batch 1/3 Cost: 1.618690\n",
      "Epoch    7/20 Batch 2/3 Cost: 1.817694\n",
      "Epoch    7/20 Batch 3/3 Cost: 2.327748\n",
      "Epoch    8/20 Batch 1/3 Cost: 2.224417\n",
      "Epoch    8/20 Batch 2/3 Cost: 1.843473\n",
      "Epoch    8/20 Batch 3/3 Cost: 2.264902\n",
      "Epoch    9/20 Batch 1/3 Cost: 2.275956\n",
      "Epoch    9/20 Batch 2/3 Cost: 2.213155\n",
      "Epoch    9/20 Batch 3/3 Cost: 3.342934\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.883463\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.687209\n",
      "Epoch   10/20 Batch 3/3 Cost: 3.354494\n",
      "Epoch   11/20 Batch 1/3 Cost: 1.752063\n",
      "Epoch   11/20 Batch 2/3 Cost: 1.138925\n",
      "Epoch   11/20 Batch 3/3 Cost: 2.948282\n",
      "Epoch   12/20 Batch 1/3 Cost: 1.979369\n",
      "Epoch   12/20 Batch 2/3 Cost: 1.044294\n",
      "Epoch   12/20 Batch 3/3 Cost: 1.921466\n",
      "Epoch   13/20 Batch 1/3 Cost: 0.476988\n",
      "Epoch   13/20 Batch 2/3 Cost: 3.877475\n",
      "Epoch   13/20 Batch 3/3 Cost: 1.891296\n",
      "Epoch   14/20 Batch 1/3 Cost: 0.836993\n",
      "Epoch   14/20 Batch 2/3 Cost: 2.073203\n",
      "Epoch   14/20 Batch 3/3 Cost: 2.821248\n",
      "Epoch   15/20 Batch 1/3 Cost: 2.286551\n",
      "Epoch   15/20 Batch 2/3 Cost: 2.601768\n",
      "Epoch   15/20 Batch 3/3 Cost: 2.472979\n",
      "Epoch   16/20 Batch 1/3 Cost: 1.277975\n",
      "Epoch   16/20 Batch 2/3 Cost: 1.695015\n",
      "Epoch   16/20 Batch 3/3 Cost: 3.361428\n",
      "Epoch   17/20 Batch 1/3 Cost: 1.242968\n",
      "Epoch   17/20 Batch 2/3 Cost: 1.847646\n",
      "Epoch   17/20 Batch 3/3 Cost: 2.384521\n",
      "Epoch   18/20 Batch 1/3 Cost: 2.074594\n",
      "Epoch   18/20 Batch 2/3 Cost: 1.565224\n",
      "Epoch   18/20 Batch 3/3 Cost: 0.774785\n",
      "Epoch   19/20 Batch 1/3 Cost: 1.008412\n",
      "Epoch   19/20 Batch 2/3 Cost: 4.044935\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.698664\n",
      "Epoch   20/20 Batch 1/3 Cost: 1.753432\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.260357\n",
      "Epoch   20/20 Batch 3/3 Cost: 1.796981\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "355f3507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.6855]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508df6d",
   "metadata": {},
   "source": [
    "# 파이토치 로지스틱 회귀 구현\n",
    "$$ H(x) = sigmoid(Wx + b) = \\frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)$$\n",
    "\n",
    "$$\\text{cost}\\left( H(x), y \\right) = -[ylogH(x) + (1-y)log(1-H(x))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7708f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f96f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15b2bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bda6c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86c7257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis) # 예측값인 H(x) 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "272df713",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5abc2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e8f10d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d8f38ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0] * torch.log(hypothesis[0]) + \n",
    "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba635525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis) + \n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa6a688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f38386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "948c1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = -(y_train * torch.log(hypothesis) + \n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e464b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01],\n",
      "        [9.9969e-01]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1b82345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c44156de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2530],\n",
      "        [1.5179]], requires_grad=True)\n",
      "tensor([-14.4819], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6028e",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀\n",
    "\n",
    "\n",
    "\n",
    "## 원핫인코딩\n",
    "* 원-핫 인코딩은 선택해야 하는 선택지의 개수만큼의 차원을 가지면서, 각 선택지의 인덱스에 해당하는 원소에는 1, 나머지 원소는 0의 값을 가지도록 하는 표현 방법\n",
    "* 강아지, 고양이, 냉장고라는 3개의 선택지\n",
    "\n",
    "강아지 = [1, 0, 0]\n",
    "\n",
    "고양이 = [0, 1, 0]\n",
    "\n",
    "냉장고 = [0, 0, 1]\n",
    "\n",
    "총 선택지는 3개였으므로 위 벡터들은 전부 3차원의 벡터를 가지고, 해당 선택지의 인덱스에만 1의 값을 가지고, 나머지 원소들은 0의 값을 가짐 이와 같은 벡터 표현을 __원핫 벡터__라고 함\n",
    "\n",
    "$$Loss\\ function = \\frac{1}{n} \\sum_i^{n} \\left(y_{i} - \\hat{y_{i}}\\right)^2$$\\\n",
    "강아지 1\n",
    "고양이 2\n",
    "냉장고 3일때,\n",
    "\n",
    "예측값이 고양이, 실제 값이 강아지 일때, $(2-1)^2=1$\n",
    "\n",
    "예측값이 냉장고, 실제 값이 강아지 일때, $(3-1)^2=4$\n",
    "\n",
    "예측값이 고양이, 실제 값이 강아지 일때, $((1,0,0)-(0,1,0))^{2} = (1-0)^{2} + (0-1)^{2} + (0-0)^{2} = 2$\n",
    "\n",
    "예측값이 냉장고, 실제 값이 강아지 일때, $((1,0,0)-(0,0,1))^{2} = (1-0)^{2} + (0-0)^{2} + (0-1)^{2} = 2$\n",
    "\n",
    "## 소프트맥스 회귀\n",
    "\n",
    "* 로지스틱 회귀\n",
    "\n",
    "![img](./img/img41.PNG)\n",
    "\n",
    "* 소프트맥스 회귀\n",
    "![img](./img/img42.PNG)\n",
    "\n",
    "## 소프트맥스 함수\n",
    "\n",
    "$$p_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}}\\ \\ for\\ i=1, 2, ... k$$\n",
    "\n",
    "$$softmax(z)=[\\frac{e^{z_{1}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{2}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{3}}}{\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = [p_{virginica}, p_{setosa}, p_{versicolor}]$$\n",
    "\n",
    "## 크로스엔트로피 함수\n",
    "\n",
    "$$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ log(p_{j}^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddee6d",
   "metadata": {},
   "source": [
    "## 소프트맥스 비용 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06afb0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "z = torch.FloatTensor([1, 2, 3])\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04e1edae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d5780b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9269, 0.3060, 0.8012, 0.5149, 0.4611],\n",
       "        [0.4840, 0.5850, 0.7357, 0.5802, 0.6525],\n",
       "        [0.0502, 0.8643, 0.9359, 0.9133, 0.8696]], requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "059e5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1569, 0.3429, 0.1406, 0.1467, 0.2129],\n",
      "        [0.1658, 0.2598, 0.1792, 0.1718, 0.2235],\n",
      "        [0.2239, 0.1097, 0.1972, 0.2150, 0.2543]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db8b3d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81aba1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot = torch.zeros_like(hypothesis) \n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62e2bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "print(y.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a021178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91afae4",
   "metadata": {},
   "source": [
    "$$cost(W) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ × (-log(p_{j}^{(i)}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2e58966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3863, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb8d88",
   "metadata": {},
   "source": [
    "## 파이토치로 소프트맥스 비용 함수 구현\n",
    "### 1. F.softmax() + torch.log() = F.log_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48f83fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3109, -1.9317, -1.4366, -1.7228, -1.7767],\n",
       "        [-1.7364, -1.6354, -1.4847, -1.6402, -1.5679],\n",
       "        [-2.3342, -1.5202, -1.4485, -1.4711, -1.5148]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c594bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3109, -1.9317, -1.4366, -1.7228, -1.7767],\n",
       "        [-1.7364, -1.6354, -1.4847, -1.6402, -1.5679],\n",
       "        [-2.3342, -1.5202, -1.4485, -1.4711, -1.5148]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cf15e",
   "metadata": {},
   "source": [
    "### 2. F.log_softmax() + F.nll_loss() = F.cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ff2a07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6437, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d163694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6437, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f1064d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6437, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원핫벡터를 넣을 필요가 없음\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ed0113cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6437, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa7b2f",
   "metadata": {},
   "source": [
    "## 소프트맥스 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "428a37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e47bd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
    "model = nn.Linear(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6aa2ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.070781\n",
      "Epoch  100/1000 Cost: 0.671531\n",
      "Epoch  200/1000 Cost: 0.577963\n",
      "Epoch  300/1000 Cost: 0.521098\n",
      "Epoch  400/1000 Cost: 0.475592\n",
      "Epoch  500/1000 Cost: 0.435517\n",
      "Epoch  600/1000 Cost: 0.398375\n",
      "Epoch  700/1000 Cost: 0.362622\n",
      "Epoch  800/1000 Cost: 0.327062\n",
      "Epoch  900/1000 Cost: 0.290850\n",
      "Epoch 1000/1000 Cost: 0.255615\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a016e",
   "metadata": {},
   "source": [
    "## 소프트맥스 회귀 클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "481b27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "522fbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "57c59902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.265973\n",
      "Epoch  100/1000 Cost: 0.655352\n",
      "Epoch  200/1000 Cost: 0.565729\n",
      "Epoch  300/1000 Cost: 0.509774\n",
      "Epoch  400/1000 Cost: 0.464932\n",
      "Epoch  500/1000 Cost: 0.425311\n",
      "Epoch  600/1000 Cost: 0.388369\n",
      "Epoch  700/1000 Cost: 0.352537\n",
      "Epoch  800/1000 Cost: 0.316633\n",
      "Epoch  900/1000 Cost: 0.280066\n",
      "Epoch 1000/1000 Cost: 0.247460\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
