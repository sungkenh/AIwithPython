{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d5852b",
   "metadata": {},
   "source": [
    "# 다층 퍼셉트론으로 손글씨 분류하기\n",
    "## 사용할 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae3d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # 시각화를 위한 맷플롯립\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits() # 1,979개의 이미지 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7581be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b231d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca33cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 1797\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 : {}'.format(len(digits.images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9760537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#상위 5개 샘플만 시각화\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:5]): # 5개의 샘플만 출력\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('sample: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c2a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 인덱스 샘플의 레이블 :  0\n",
      "1 번 인덱스 샘플의 레이블 :  1\n",
      "2 번 인덱스 샘플의 레이블 :  2\n",
      "3 번 인덱스 샘플의 레이블 :  3\n",
      "4 번 인덱스 샘플의 레이블 :  4\n"
     ]
    }
   ],
   "source": [
    "#상위 5개 샘플 레이블 확인\n",
    "for i in range(5):\n",
    "    print(i,'번 인덱스 샘플의 레이블 : ',digits.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e367868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#digits.images는 모든 샘플을 8 × 8 행렬로 저장하고 있음\n",
    "#digts.data는 64차원의 벡터로 변환해서 저장하고 있음\n",
    "print(digits.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74616a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data # 이미지. 즉, 특성 행렬\n",
    "Y = digits.target # 각 이미지에 대한 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9d6f9",
   "metadata": {},
   "source": [
    "## 분류기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e528cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071eba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(64, 32), # input_layer = 64, hidden_layer1 = 32\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16), # hidden_layer2 = 32, hidden_layer3 = 16\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10) # hidden_layer3 = 16, output_layer = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3f5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32a43e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # 이 비용 함수는 소프트맥스 함수를 포함하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6744a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14757411",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56576887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.281738\n",
      "Epoch   10/100 Cost: 0.237729\n",
      "Epoch   20/100 Cost: 0.203228\n",
      "Epoch   30/100 Cost: 0.176478\n",
      "Epoch   40/100 Cost: 0.155262\n",
      "Epoch   50/100 Cost: 0.137892\n",
      "Epoch   60/100 Cost: 0.123714\n",
      "Epoch   70/100 Cost: 0.111961\n",
      "Epoch   80/100 Cost: 0.101911\n",
      "Epoch   90/100 Cost: 0.093155\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X) # forwar 연산\n",
    "    loss = loss_fn(y_pred, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, 100, loss.item()))\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f84e8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f12b404d460>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0dd4a",
   "metadata": {},
   "source": [
    "# MLP로 MNIST 분류하기\n",
    "*  앞서 우리가 실습시간에 진행했던, 소프트맥스 회귀 또한 인공 신경망이라고 볼 수 있음\n",
    "* 입력층과 출력층만 존재하므로 소프트맥스 함수를 활성화 함수로 사용한 '단층 퍼셉트론'임\n",
    "* 이 예제에서는 은닉층을 추가로 넣어 다층 퍼셉트론을 구현하고, 딥 러닝을 통해서 MNIST 데이터를 분류함\n",
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5847c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37811897",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62c6ce39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995       0.0       0.0       0.0       0.0       0.0  \n",
       "69996       0.0       0.0       0.0       0.0       0.0  \n",
       "69997       0.0       0.0       0.0       0.0       0.0  \n",
       "69998       0.0       0.0       0.0       0.0       0.0  \n",
       "69999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e9aacc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        0\n",
       "2        4\n",
       "3        1\n",
       "4        9\n",
       "        ..\n",
       "69995    2\n",
       "69996    3\n",
       "69997    4\n",
       "69998    5\n",
       "69999    6\n",
       "Name: class, Length: 70000, dtype: category\n",
       "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "226d5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target = mnist.target.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00309904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data / 255  # 0-255값을 [0,1] 구간으로 정규화\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f73934d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a93003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef927bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 이미지 데이터의 레이블은 5이다\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X.to_numpy()[0].reshape(28, 28), cmap='gray')\n",
    "print(\"이 이미지 데이터의 레이블은 {:.0f}이다\".format(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4a446",
   "metadata": {},
   "source": [
    "## 훈련데이터 테스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34d44048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df5faa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)\n",
    "\n",
    "X_train = torch.Tensor(X_train.to_numpy())\n",
    "X_test = torch.Tensor(X_test.to_numpy())\n",
    "y_train = torch.LongTensor(y_train.to_numpy())\n",
    "y_test = torch.LongTensor(y_test.to_numpy())\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3738d6",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a80a67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(28*28*1, 100))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(100, 100))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(100, 10))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba101339",
   "metadata": {},
   "source": [
    "## 학습을 위한 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a656ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# 오차함수 선택\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 가중치를 학습하기 위한 최적화 기법 선택\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e984bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()  # 신경망을 학습 모드로 전환\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 학습을 수행\n",
    "    for data, targets in loader_train:\n",
    "\n",
    "        optimizer.zero_grad()  # 경사를 0으로 초기화\n",
    "        outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "        loss = loss_fn(outputs, targets)  # 출력과 훈련 데이터 정답 간의 오차를 계산\n",
    "        loss.backward()  # 오차를 역전파 계산\n",
    "        optimizer.step()  # 역전파 계산한 값으로 가중치를 수정\n",
    "\n",
    "    print(\"epoch{}：완료\\n\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ff3052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # 신경망을 추론 모드로 전환\n",
    "    correct = 0\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
    "    with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
    "        for data, targets in loader_test:\n",
    "\n",
    "            outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "\n",
    "            # 추론 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum()  # 정답과 일치한 경우 정답 카운트를 증가\n",
    "\n",
    "    # 정확도 출력\n",
    "    data_num = len(loader_test.dataset)  # 데이터 총 건수\n",
    "    print('\\n테스트 데이터에서 예측 정확도: {}/{} ({:.0f}%)\\n'.format(correct,\n",
    "                                                   data_num, 100. * correct / data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c83bdf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 데이터에서 예측 정확도: 1463/10000 (15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4cd0d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0：완료\n",
      "\n",
      "epoch1：완료\n",
      "\n",
      "epoch2：완료\n",
      "\n",
      "\n",
      "테스트 데이터에서 예측 정확도: 9599/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92ea486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 : 2\n",
      "이 이미지 데이터의 정답 레이블은 2입니다\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2018\n",
    "\n",
    "model.eval()  # 신경망을 추론 모드로 전환\n",
    "data = X_test[index]\n",
    "output = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "_, predicted = torch.max(output.data, 0)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
    "\n",
    "print(\"예측 결과 : {}\".format(predicted))\n",
    "\n",
    "X_test_show = (X_test[index]).numpy()\n",
    "plt.imshow(X_test_show.reshape(28, 28), cmap='gray')\n",
    "print(\"이 이미지 데이터의 정답 레이블은 {:.0f}입니다\".format(y_test[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13afad55",
   "metadata": {},
   "source": [
    "# 과적합(Overfitting)을 해결하는 방법\n",
    "* 학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈임 \n",
    "* 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터. 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않음\n",
    "* 이는 모델이 학습 데이터를 불필요할정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있음\n",
    "* 인공신경망에서 과적합을 막는 방법을 소개함\n",
    "\n",
    "## 1.데이터 양 늘리기\n",
    "* 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하기 되므로 과적합 현상이 발생할 확률이 늘어납니다. 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있습니다.\n",
    "\n",
    "만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 합니다. 이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식시킵니다.\n",
    "\n",
    "## 2. 모델 복잡도 줄이기\n",
    "* 인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정됨\n",
    "* 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa46291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nnReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87218e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2개의 선형 레이어로 레이어 축소\n",
    "class Architecture1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Architecture1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c977478",
   "metadata": {},
   "source": [
    "## 3. 가중치 규제(Regularization) 적용하기\n",
    "* 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높음\n",
    "* 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제(Regularizaiton)가 있음\n",
    "    * L1 규제 : 가중치 w들의 절대값 합계를 비용 함수에 추가합니다. L1 노름이라고도 합니다.\n",
    "    * L2 규제 : 모든 가중치 w들의 제곱합을 비용 함수에 추가합니다. L2 노름이라고도 합니다.\n",
    "    * L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda \\mid w \\mid$를 더 한 값을 비용 함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\frac{1}{2} \\lambda w^2$를 더 한 값을 비용 함수로 함\n",
    "    * 여기서 $\\lambda$는 규제의 강도를 정하는 하이퍼파라미터로, 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미\n",
    "\n",
    "* 두 가지 규제 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아져야 한다는 특징이 있음\n",
    "* L1 규제로 예를 들어봅시다. L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 합니다. 이렇게 되면, 가중치 w의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 됨\n",
    "\n",
    "* 파이토치에서는 옵티마이저의 weight_decay 매개변수를 설정하므로서 L2 규제를 적용할 수 있음(weight_decay 기본값은 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f748ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Architecture1(10, 20, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb0439",
   "metadata": {},
   "source": [
    "## 4. 드롭아웃(Dropout)\n",
    "* 드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법\n",
    "![img](https://pbs.twimg.com/media/Du42CMeUYAAJufd.jpg)\n",
    "![img](https://wikidocs.net/images/page/60751/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG)\n",
    "\n",
    "* 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2892ed",
   "metadata": {},
   "source": [
    "# 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\n",
    "* 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생 (기울기 소실: Gradient Vanishing)\n",
    "* 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산(기울기 폭주: Gradient Exploding)\n",
    "\n",
    "## 1. 활성화 함수 ReLU, ReakyReLU\n",
    "* 시그모이드 함수를 사용하는 경우 기울기가 소실되는 문제가 있었음\n",
    "\n",
    "## 2. 가중치 초기화\n",
    "* 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 함(가중치 초기화만 적절히 해줘도 기울기 소실 문제를 완화)\n",
    "* 1. 세이비어 초기화\n",
    "     * 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했음\n",
    "     * 이 방법은 제안한 사람의 이름을 따서 __세이비어(Xavier Initialization) 초기화__라고 함\n",
    "     * 이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세움\n",
    "     * 균등 분포를 사용하여 초기화 할 경우,\n",
    "     $$ W \\sim Uniform(-\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }}, +\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }}) $$\n",
    "     * 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막음\n",
    "     * 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않음\n",
    "     * ReLU 함수 또는 ReLU의 변형 함수들을 활성화 함수로 사용할 경우에는 다른 초기화 방법을 사용하는 것이 좋은데, 이를 __He 초기화(He initialization)__라고 함\n",
    "     \n",
    "* 2. HE 초기화\n",
    "    * 세이비어랑 유사하나 다음 층의 뉴런의 수를 반영하지 않음\n",
    "    $$W\\sim Uniform(- \\sqrt{\\frac { 6 }{ { n }_{ in } } } , \\space\\space + \\sqrt{\\frac { 6 }{ { n }_{ in } } } )$$\n",
    "    * 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적\n",
    "    * ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2efb1fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1430,  0.1348, -0.0572,  ...,  0.0947, -0.0364,  0.0021],\n",
       "        [ 0.0440,  0.0786, -0.0142,  ..., -0.0941, -0.1386,  0.0833],\n",
       "        [-0.0906,  0.0026,  0.1323,  ...,  0.0378, -0.1111, -0.0634],\n",
       "        ...,\n",
       "        [ 0.0641,  0.1183,  0.0827,  ...,  0.0386, -0.0998, -0.0432],\n",
       "        [ 0.0774, -0.0392, -0.0573,  ..., -0.0440,  0.0818,  0.1077],\n",
       "        [-0.1232,  0.0540,  0.0363,  ...,  0.0879, -0.1359,  0.1266]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# Xavier initialization\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd911f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1212,  0.0615, -0.0949,  ...,  0.0084,  0.1052,  0.0797],\n",
       "        [ 0.0451, -0.1221,  0.0124,  ...,  0.1040,  0.1510,  0.1035],\n",
       "        [-0.1032, -0.1191,  0.1069,  ..., -0.1301,  0.1321, -0.0617],\n",
       "        ...,\n",
       "        [-0.0613, -0.1382, -0.0865,  ..., -0.0806, -0.1394,  0.0266],\n",
       "        [-0.0936,  0.0856, -0.0184,  ..., -0.1202, -0.0792, -0.1105],\n",
       "        [ 0.0024, -0.1458,  0.1389,  ..., -0.1142,  0.0249, -0.0005]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HE initialization\n",
    "torch.nn.init.kaiming_uniform_(linear1.weight)\n",
    "torch.nn.init.kaiming_uniform_(linear2.weight)\n",
    "torch.nn.init.kaiming_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e5f7e",
   "metadata": {},
   "source": [
    "## 배치 정규화(Batch Normalization)\n",
    "* ReLU 계열의 함수와 He 초기화를 사용하는 것만으로도 어느 정도 기울기 소실과 폭주를 완화시킬 수 있지만, 이 두 방법을 사용하더라도 훈련 중에 언제든 다시 발생할 수 있음\n",
    "* 울기 소실이나 폭주를 예방하는 또 다른 방법은 배치 정규화(Batch Normalization)임\n",
    "* 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 __평균과 분산으로 정규화__하여 학습을 효율적으로 만듬\n",
    "* 배치 정규화는 배치 단위로 정규화하는 것을 말하며, 각 층에서 활성화 함수를 통과하기 전에 수행됨\n",
    "* 입력에 대해 평균을 0으로 만들고, 정규화를 함\n",
    "Input: 미니배치 $B = \\{{x}^{(1)}, {x}^{(2)}, ..., {x}^{(m)}\\}$\n",
    "Output: $y^{(i)} = BN_{γ, β}(x^{(i)})$\n",
    "$$ μ_{B} ← \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} \\text{ # 미니 배치에 대한 평균}$$\n",
    "$$σ^{2}_{B} ← \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - μ_{B})^{2}\\text{ # 미니 배치에 대한 분산}$$\n",
    "$$\\hat{x}^{(i)} ← \\frac{x^{(i)} - μ_{B}}{\\sqrt{σ^{2}_{B}+ε}}\\text{ # 정규화}$$\n",
    "$$y^{(i)} ← γ\\hat{x}^{(i)} + β = BN_{γ, β}(x^{(i)}) \\text{ # 스케일 조정과 시프트}$$\n",
    "* $ε$은 분모가 0이 되는 것을 막는 작은 수. 보편적으로 $10^{-5}$\n",
    "* $γ$는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상\n",
    "* $β$는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상\n",
    "\n",
    "* 배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화\n",
    "\n",
    "* 장점\n",
    "    * 시그모이드나, 하이퍼볼릭탄젠트의 기울기 소실 문제 개선\n",
    "    * 가중치 초기화에 훨씬 덜 민감\n",
    "    * 훨씬 큰 학습률을 사용할 수 있어 학습 속도 개선\n",
    "* 단점\n",
    "    *  모델을 더 복잡하게 함\n",
    "    *  추가 계산이 필요하므로 테스트 예측 시에도 시간이 느려짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6c6acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 32, bias=True).to(device)\n",
    "        self.linear2 = nn.Linear(32, 32, bias=True).to(device)\n",
    "        self.linear3 = nn.Linear(32, 10, bias=True).to(device)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        \n",
    "        self.model = nn.Sequential(self.linear1, self.bn1, self.relu,\n",
    "                                   self.linear2, self.bn2, self.relu,\n",
    "                                   self.linear3).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d92f7f",
   "metadata": {},
   "source": [
    "## 층 정규화(Layer Normalization)\n",
    "* 층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보겠습니다. 다음은 $m$이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여줌\n",
    "* 미니 배치란 동일한 특성(feature) 개수들을 가진 다수의 샘플들을 의미함\n",
    "* 배치 정규화\n",
    "![img](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG)\n",
    "* 층 정규화\n",
    "![img](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9c70c89",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-21487a4a9fc5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-21487a4a9fc5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lass block(nn.Module):\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lass block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels):\n",
    "        super(block, self).__init__()\n",
    "        ln = nn.GroupNorm(1, in_channel, eps=1e-08)\n",
    "        ...\n",
    "    def forward(self,x):\n",
    "        out = ln(x)  #[B, N, T] -> [B, N, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
